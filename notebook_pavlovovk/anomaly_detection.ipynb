{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../data/cleaned/South_East_Asia_Social_Media_MentalHealth_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['likes_received', 'comments_received', 'shares_received']\n",
    "data_selected = data_df[selected_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Contamination is the expected fraction of anomalies\n",
    "iso_forest.fit(data_scaled)\n",
    "\n",
    "# Predict anomalies\n",
    "data_df['anomaly_score'] = iso_forest.decision_function(data_scaled)  # Scores: negative values are more anomalous\n",
    "data_df['is_anomaly'] = iso_forest.predict(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = data_df[data_df['is_anomaly'] == -1]\n",
    "normal = data_df[data_df['is_anomaly'] == 1]\n",
    "print(f\"Total anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Percentage of anomalies: {len(anomalies) / len(data_df) * 100:.2f}%\")\n",
    "\n",
    "# View a few anomalies\n",
    "anomalies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing specific anomalies\n",
    "sample_anomaly = anomalies[selected_columns].describe()\n",
    "print(\"Summary statistics for anomalies:\")\n",
    "print(sample_anomaly)\n",
    "\n",
    "# Compare with normal samples\n",
    "sample_normal = normal[selected_columns].describe()\n",
    "print(\"\\nSummary statistics for normal data:\")\n",
    "print(sample_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "#  Local Outlier Factor (LOF) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Initialize the LOF model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "data_df['lof_anomaly'] = lof.fit_predict(data_scaled)  # -1 indicates anomaly\n",
    "\n",
    "# Compare LOF anomalies with Isolation Forest\n",
    "lof_anomalies = data_df[data_df['lof_anomaly'] == -1]\n",
    "print(f\"Total anomalies detected by LOF: {len(lof_anomalies)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
